{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyAnYlOlxHrPPSZ6Ch4bSv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afshari-maryam/Adversarial-domain-adaptation/blob/main/FineTune_Wav2vec2_Voxceleb_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to finetune wav2vec2 model with voxceleb dataset?**\n",
        "ChatGPT conversatio :)"
      ],
      "metadata": {
        "id": "ZvUswwPXO9Wr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetuning the wav2vec2 model with the VoxCeleb dataset involves the following steps:\n",
        "\n",
        "1- Preprocess the VoxCeleb dataset to create training and validation sets. This typically involves extracting audio files from the dataset and possibly converting them to a suitable format (such as WAV), dividing the dataset into training and validation sets, and creating a list of file paths and corresponding labels for each set.\n",
        "\n",
        "2-Set up a PyTorch DataLoader for each set (training and validation). The DataLoader will be used to iterate over the dataset during training and evaluation.\n",
        "\n",
        "3-Load the wav2vec2 model and its associated tokenizer from the Hugging Face transformers library.\n",
        "\n",
        "4-Define a suitable loss function and optimizer for the finetuning task.\n",
        "\n",
        "5-Set up a training loop to iterate over the training set using the DataLoader and update the model parameters using the optimizer. You will also want to track the model's performance on the validation set and save the model checkpoint with the best validation performance\n",
        "\n",
        "6- Optionally, you can also use the wav2vec2 model for inference on new audio samples by encoding the audio with the tokenizer and passing the encoded samples through the model.\n",
        "\n",
        "Here is some example code that demonstrates how to finetune the wav2vec2 model with the VoxCeleb dataset:"
      ],
      "metadata": {
        "id": "EVHTwSliOvPF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRboqEY-OoXG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2Tokenizer\n",
        "\n",
        "# Preprocess the VoxCeleb dataset to create training and validation sets\n",
        "# This step is not shown in the example code, as it depends on how you have the dataset stored\n",
        "train_set = ...\n",
        "validation_set = ...\n",
        "\n",
        "# Set up a DataLoader for the training set\n",
        "train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "\n",
        "# Set up a DataLoader for the validation set\n",
        "validation_dataloader = DataLoader(validation_set, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load the wav2vec2 model and its associated tokenizer\n",
        "model = Wav2Vec2Model.from_pretrained('wav2vec2-large-960h-clean')\n",
        "tokenizer = Wav2Vec2Tokenizer.from_pretrained('wav2vec2-large-960h-clean')\n",
        "\n",
        "# Set up a loss function and an optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Set up a training loop\n",
        "best_validation_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Iterate over the training set\n",
        "    for inputs, labels in train_dataloader:\n",
        "        # Encode the audio with the tokenizer\n",
        "        input_ids = tokenizer(inputs, return_tensors='pt').input_ids\n",
        "\n",
        "        # Pass the input through the model\n",
        "        logits = model(input_ids)[0]\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        # Backpropagate the gradients and update the model parameters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # Set the model to evaluation mode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to Preprocess the VoxCeleb dataset to create training and validation sets.**"
      ],
      "metadata": {
        "id": "NTpoDqlCP0u1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Try1 : **\n",
        "Preprocessing the VoxCeleb dataset to create training and validation sets typically involves the following steps:\n",
        "\n",
        "1- Download the VoxCeleb dataset from the official website (http://www.robots.ox.ac.uk/~vgg/data/voxceleb/) or from a mirror site. The dataset consists of a collection of audio files and associated metadata in various formats.\n",
        "\n",
        "2- Extract the audio files from the dataset and possibly convert them to a suitable format (such as WAV). This can be done using a tool such as ffmpeg.\n",
        "\n",
        "3- Divide the dataset into training and validation sets. This can be done randomly or by following a predetermined split.\n",
        "\n",
        "4- Create a list of file paths and corresponding labels for each set. The labels can be the speaker IDs or some other metadata associated with the audio files.\n",
        "\n",
        "Here is some example code that demonstrates how to preprocess the VoxCeleb dataset and create training and validation sets:"
      ],
      "metadata": {
        "id": "i91pjELKP3ED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "# Set the directories for the VoxCeleb dataset and the output training and validation sets\n",
        "voxceleb_dir = '/path/to/voxceleb'\n",
        "train_dir = '/path/to/train'\n",
        "validation_dir = '/path/to/validation'\n",
        "\n",
        "# Extract the audio files from the dataset and convert them to WAV format\n",
        "for speaker_dir in os.listdir(voxceleb_dir):\n",
        "    speaker_id = speaker_dir.split('_')[1]\n",
        "    for audio_file in os.listdir(os.path.join(voxceleb_dir, speaker_dir)):\n",
        "        audio_path = os.path.join(voxceleb_dir, speaker_dir, audio_file)\n",
        "        wav_path = os.path.join(voxceleb_dir, speaker_dir, audio_file.replace('.flac', '.wav'))\n",
        "        os.system(f'ffmpeg -i {audio_path} -acodec pcm_s16le -ac 1 -ar 16000 {wav_path}')\n",
        "\n",
        "# Create a list of file paths and labels for the VoxCeleb dataset\n",
        "files = []\n",
        "for speaker_dir in os.listdir(voxceleb_dir):\n",
        "    speaker_id = speaker_dir.split('_')[1]\n",
        "    for wav_file in os.listdir(os.path.join(voxceleb_dir, speaker_dir)):\n",
        "        if wav_file.endswith('.wav'):\n",
        "            file_path = os.path.join(voxceleb_dir, speaker_dir, wav_file)\n",
        "            files.append((file_path, speaker_id))\n",
        "\n",
        "# Shuffle the list of file paths and labels\n",
        "random.shuffle(files)\n",
        "\n",
        "# Split the list into training and validation sets\n",
        "num_train = int(0.8 * len(files))\n",
        "train_files = files[:num_train]\n",
        "validation_files = files[num_train:]\n",
        "\n",
        "# Create the directories for the training and validation sets\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "\n",
        "# Copy the audio files to the training and validation directories\n",
        "for file_path, label in train_files:\n",
        "    destination_path = os.path.join(train_dir, f'\n"
      ],
      "metadata": {
        "id": "qjPHa6tBP1yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Try2  **\n",
        "To preprocess the VoxCeleb dataset to create training and validation sets, you will need to perform the following steps:\n",
        "\n",
        "Download the VoxCeleb dataset and extract the audio files. The VoxCeleb dataset consists of audio files in MP3 format, which may need to be converted to a different format (such as WAV) depending on the requirements of the model you are using.\n",
        "\n",
        "Divide the dataset into training and validation sets. You can do this by randomly selecting a certain percentage of the audio files for the training set and the remaining files for the validation set.\n",
        "\n",
        "Create a list of file paths and corresponding labels for each set. You will need to create a list of file paths for each audio file in the training and validation sets, along with the corresponding labels (i.e., the speaker IDs). This list will be used to create a PyTorch Dataset object for each set.\n",
        "\n",
        "Here is some example code that demonstrates how to preprocess the VoxCeleb dataset and create a list of file paths and labels for the training set:"
      ],
      "metadata": {
        "id": "gNcyDzQ9SY8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "# Set the path to the VoxCeleb dataset\n",
        "voxceleb_path = '/path/to/voxceleb'\n",
        "\n",
        "# Set the percentage of the dataset to use for the training set\n",
        "train_percentage = 0.8\n",
        "\n",
        "# Get the list of audio files in the dataset\n",
        "audio_files = os.listdir(voxceleb_path)\n",
        "\n",
        "# Shuffle the list of audio files\n",
        "random.shuffle(audio_files)\n",
        "\n",
        "# Calculate the number of audio files to use for the training set\n",
        "num_train_files = int(len(audio_files) * train_percentage)\n",
        "\n",
        "# Create a list of file paths and labels for the training set\n",
        "train_set = []\n",
        "for audio_file in audio_files[:num_train_files]:\n",
        "    file_path = os.path.join(voxceleb_path, audio_file)\n",
        "    label = audio_file.split('_')[0]\n",
        "    train_set.append((file_path, label))\n"
      ],
      "metadata": {
        "id": "ZgnZU4zPShA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Try**"
      ],
      "metadata": {
        "id": "5bNH_T32dTR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**how to create training and validation sets from the VoxCeleb dataset and set up PyTorch DataLoaders for each set.**"
      ],
      "metadata": {
        "id": "dyB8_cucddFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Try3**\n",
        "Preprocessing the VoxCeleb dataset to create training and validation sets typically involves the following steps:\n",
        "\n",
        "Download and extract the VoxCeleb dataset. The VoxCeleb dataset is a large dataset of audio files and corresponding labels (speaker identities) that can be used for training and evaluating automatic speaker verification systems. The dataset is available for download from the VoxCeleb website (http://www.robots.ox.ac.uk/~vgg/data/voxceleb/).\n",
        "\n",
        "Convert the audio files to a suitable format. The VoxCeleb dataset contains audio files in various formats, such as WAV, MP3, and Ogg Vorbis. You may need to convert the audio files to a specific format that is compatible with your model and training pipeline. For example, you may want to convert the audio files to WAV format to use with the wav2vec2 model.\n",
        "\n",
        "Divide the dataset into training and validation sets. You can do this by randomly selecting a subset of the audio files for training and the remaining files for validation. You can also use a stratified sampling approach to ensure that the training and validation sets have a similar distribution of speaker identities.\n",
        "\n",
        "Create a list of file paths and corresponding labels for each set. You will need a list of file paths and labels for each audio file in the training and validation sets in order to use the dataset with a PyTorch DataLoader. You can create these lists by iterating over the audio files in the training and validation sets and extracting the file paths and labels for each file.\n",
        "\n",
        "Here is some example code that demonstrates how to preprocess the VoxCeleb dataset and create training and validation sets:"
      ],
      "metadata": {
        "id": "yQrzx087cMio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "# Set the root directory of the VoxCeleb dataset\n",
        "voxceleb_root = '/path/to/voxceleb'\n",
        "\n",
        "# Set the directories for the training and validation sets\n",
        "train_dir = os.path.join(voxceleb_root, 'train')\n",
        "validation_dir = os.path.join(voxceleb_root, 'validation')\n",
        "\n",
        "# Set the file extension for the audio files\n",
        "audio_extension = '.wav'\n",
        "\n",
        "# Create lists of file paths and labels for the training and validation sets\n",
        "train_paths = []\n",
        "train_labels = []\n",
        "validation_paths = []\n",
        "validation_labels = []\n",
        "\n",
        "# Iterate over the directories and extract the file paths and labels\n",
        "for root, dirs, files in os.walk(train_dir):\n",
        "    for file in files:\n",
        "        if file.endswith(audio_extension):\n",
        "            file_path = os.path.join(root, file)\n",
        "            label = file.split('_')[0]\n",
        "            train_paths.append(file_path)\n",
        "            train_labels.append(label)\n",
        "\n",
        "for root, dirs, files in os.walk(validation_dir):\n",
        "    for file in files:\n",
        "        if file.endswith(audio_extension):\n",
        "            file_path = os.path.join(root, file)\n",
        "            label = file.split('_')[0]\n",
        "            validation_paths.append(file_path)\n",
        "            validation_labels.append(label)\n",
        "\n",
        "# Shuffle the training and validation sets\n",
        "random.shuffle(train_paths)\n",
        "random.shuffle(validation_paths)\n",
        "\n",
        "# You can now use the lists of file paths and labels to\n",
        "# Create a custom dataset class for the VoxCeleb dataset\n",
        "class VoxCelebDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, file_paths, labels):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        file_path = self.file_paths[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # Load the audio file and return it along with the label\n",
        "        audio, _ = torchaudio.load(file_path)\n",
        "        return audio, label\n",
        "\n",
        "# Create training and validation datasets using the custom dataset class\n",
        "train_set = VoxCelebDataset(train_paths, train_labels)\n",
        "validation_set = VoxCelebDataset(validation_paths, validation_labels)\n",
        "\n",
        "# Set up a DataLoader for the training set\n",
        "train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "\n",
        "# Set up a DataLoader for the validation set\n",
        "validation_dataloader = DataLoader(validation_set, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "iWvTp2kicQee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**finetune the wav2vec2 model with the VoxCeleb dataset**"
      ],
      "metadata": {
        "id": "rL4QNq0odiDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I apologize for the misunderstanding. The previous example code showed **how to create training and validation sets from the VoxCeleb dataset and set up PyTorch DataLoaders for each set.**\n",
        "\n",
        "To **finetune the wav2vec2 model with the VoxCeleb dataset**, you will need to perform additional steps such as loading the wav2vec2 model and its associated tokenizer, defining a suitable loss function and optimizer, and setting up a training loop to iterate over the training set and update the model parameters.\n",
        "\n",
        "Here is an example of how to finetune the wav2vec2 model with the VoxCeleb dataset using the DataLoaders created in the previous example:"
      ],
      "metadata": {
        "id": "9JcjBs2wdW9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2Tokenizer\n",
        "\n",
        "# Load the wav2vec2 model and its associated tokenizer\n",
        "model = Wav2Vec2Model.from_pretrained('wav2vec2-large-960h-clean')\n",
        "tokenizer = Wav2Vec2Tokenizer.from_pretrained('wav2vec2-large-960h-clean')\n",
        "\n",
        "# Set up a loss function and an optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Set up a training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Iterate over the training set\n",
        "    for inputs, labels in train_dataloader:\n",
        "        # Encode the audio with the tokenizer\n",
        "        input_ids = tokenizer(inputs, return_tensors='pt').input_ids\n",
        "\n",
        "        # Pass the input through the model\n",
        "        logits = model(input_ids)[0]\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        # Backpropagate the gradients and update the model parameters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize variables to track the average loss and accuracy\n",
        "    avg_loss = 0.0\n",
        "    avg_accuracy = 0.0\n",
        "    num_examples = 0\n",
        "\n",
        "    # Iterate over the validation set\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            # Encode the audio with the tokenizer\n",
        "            input_ids = tokenizer(inputs, return_tensors='pt').input_ids\n",
        "\n",
        "            # Pass the input through the model\n",
        "            logits = model(input_ids)[0]\n",
        "\n",
        "            # Calculate the loss and accuracy\n",
        "            loss = loss_fn(logits, labels)\n",
        "            accuracy = (torch.argmax(logits, dim=1) == labels).float().mean()\n",
        "\n",
        "            # Update the running averages\n",
        "            avg_loss += loss.item() * inputs.size(0)\n",
        "            avg_accuracy += accuracy.item() * inputs.size(0)\n",
        "            num_examples += inputs.size(0)\n",
        "\n",
        "    # Calculate the average loss and accuracy for the epoch\n",
        "    avg_loss /= num_examples\n",
        "    avg_accuracy /= num_examples\n",
        "\n",
        "    # Print the results for the epoch\n",
        "    print(f'Epoch: {epoch+1} Loss:\n"
      ],
      "metadata": {
        "id": "mUNQoey8dnne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Set up a training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Iterate over the training set\n",
        "    for inputs, labels in train_dataloader:\n",
        "        # Encode the audio with the tokenizer\n",
        "        input_ids = tokenizer(inputs, return_tensors='pt').input_ids\n",
        "\n",
        "        # Pass the input through the model\n",
        "        logits = model(input_ids)[0]\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        # Backpropagate the gradients and update the model parameters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize variables to track the average loss and accuracy\n",
        "    avg_loss = 0.0\n",
        "    avg_accuracy = 0.0\n",
        "    num_examples = 0\n",
        "\n",
        "    # Iterate over the validation set\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataloader:\n",
        "            # Encode the audio with the tokenizer\n",
        "            input_ids = tokenizer(inputs, return_tensors='pt').input_ids\n",
        "\n",
        "            # Pass the input through the model\n",
        "            logits = model(input_ids)[0]\n",
        "\n",
        "            # Calculate the loss and accuracy\n",
        "            loss = loss_fn(logits, labels)\n",
        "            accuracy = (torch.argmax(logits, dim=1) == labels).float().mean()\n",
        "\n",
        "            # Update the running averages\n",
        "            avg_loss += loss.item() * inputs.size(0)\n",
        "            avg_accuracy += accuracy.item() * inputs.size(0)\n",
        "            num_examples += inputs.size(0)\n",
        "\n",
        "#    # Calculate the average loss and accuracy for the epoch\n",
        "#    avg_loss /= num_ex\n",
        "# Calculate the average loss and accuracy for the epoch\n",
        "    avg_loss /= num_examples\n",
        "    avg_accuracy /= num_examples\n",
        "\n",
        "    # Print the results for the epoch\n",
        "    print(f'Epoch: {epoch+1} Loss: {avg_loss:.4f} Accuracy: {avg_accuracy:.4f}')\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'wav2vec2_finetuned.pth')"
      ],
      "metadata": {
        "id": "qCv8wXWHds7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will finetune the wav2vec2 model on the VoxCeleb dataset for a specified number of epochs, using the training and validation DataLoaders. It will print the average loss and accuracy for each epoch, and save the model checkpoint with the best validation performance.\n",
        "\n",
        "You can"
      ],
      "metadata": {
        "id": "KIQoObwheTT2"
      }
    }
  ]
}