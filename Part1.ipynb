{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtY7kZqkeWMa1CRJH6jMmk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afshari-maryam/Adversarial-domain-adaptation/blob/main/Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using pretrained model :\n",
        "pipeline function"
      ],
      "metadata": {
        "id": "iZi2rfosy7Lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This list is far from comprehensive, and is just meant to highlight a few of the different kinds of Transformer models. Broadly, they can be grouped into three categories:\n",
        "\n",
        "GPT-like (also called auto-regressive Transformer models)\n",
        "BERT-like (also called auto-encoding Transformer models)\n",
        "BART/T5-like (also called sequence-to-sequence Transformer models)"
      ],
      "metadata": {
        "id": "41uZQMG31mtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is called causal language modeling because the output depends on the past and present inputs, but not the future ones."
      ],
      "metadata": {
        "id": "uqrxWocs1-7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another example is masked language modeling, in which the model predicts a masked word in the sentence."
      ],
      "metadata": {
        "id": "AN-mu2PR1_5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each of these parts can be used independently, depending on the task:\n",
        "\n",
        "**Encoder-only model**s: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.\n",
        "**Decoder-only models**: Good for generative tasks such as text generation.\n",
        "**Encoder-decoder models** or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization."
      ],
      "metadata": {
        "id": "lQ7M6QG-3ZE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These terms all have slightly different meanings:\n",
        "\n",
        "**Architecture:** This is the skeleton of the model — the definition of each layer and each operation that happens within the model.\n",
        "**Checkpoints:** These are the weights that will be loaded in a given architecture.\n",
        "**Model:** This is an umbrella term that isn’t as precise as “architecture” or “checkpoint”: it can mean both. This course will specify architecture or checkpoint when it matters to reduce ambiguity.\n",
        "**For example**, BERT is an architecture while **bert-base-cased**, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say “the BERT model” and “the bert-base-cased model.”"
      ],
      "metadata": {
        "id": "ndSsxbx0_VOP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNBUfxGSw1aG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finetune pretrained model :\n",
        "[mask] function :)"
      ],
      "metadata": {
        "id": "HGRS6nJ5zDnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pretraine Model :"
      ],
      "metadata": {
        "id": "6s2cL0YLzl5H"
      }
    }
  ]
}